{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "HM4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMAkgcEQmmNj"
      },
      "source": [
        "# Home 4: Build a CNN for image recognition.\n",
        "\n",
        "### Name: Ayesha Parveen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7Fie2RwmmNp"
      },
      "source": [
        "## 0. You will do the following:\n",
        "\n",
        "1. Read, complete, and run the code.\n",
        "\n",
        "2. **Make substantial improvements** to maximize the accurcy.\n",
        "    \n",
        "3. Convert the .IPYNB file to .HTML file.\n",
        "\n",
        "    * The HTML file must contain the code and the output after execution.\n",
        "    \n",
        "    * Missing **the output after execution** will not be graded.\n",
        "    \n",
        "4. Upload this .HTML file to your Google Drive, Dropbox, or Github repo. (If you submit the file to Google Drive or Dropbox, you must make the file \"open-access\". The delay caused by \"deny of access\" may result in late penalty.)\n",
        "\n",
        "4. Submit the link to this .HTML file to Canvas.\n",
        "\n",
        "    * Example: https://github.com/wangshusen/CS583-2020S/blob/master/homework/HM4/HM4.html\n",
        "\n",
        "\n",
        "## Requirements:\n",
        "\n",
        "1. You can use whatever CNN architecture, including VGG, Inception, and ResNet. However, you must build the networks layer by layer. You must NOT import the archetectures from ```keras.applications```.\n",
        "\n",
        "2. Make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer.\n",
        "\n",
        "3. If you want to regularize a ```Conv```/```Dense``` layer, you should place a ```Dropout``` layer **before** the ```Conv```/```Dense``` layer.\n",
        "\n",
        "4. An accuracy above 70% is considered reasonable. An accuracy above 80% is considered good. Without data augmentation, achieving 80% accuracy is difficult.\n",
        "\n",
        "\n",
        "## Google Colab\n",
        "\n",
        "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option.\n",
        "\n",
        "- Keep in mind that you must download it as an IPYNB file and then use IPython Notebook to convert it to HTML.\n",
        "\n",
        "- Also keep in mind that the IPYNB and HTML files must contain the outputs. (Otherwise, the instructor will not be able to know the correctness and performance.) Do the followings to keep the outputs.\n",
        "\n",
        "- In Colab, go to ```Runtime``` --> ```Change runtime type``` --> Do NOT check ```Omit code cell output when saving this notebook```. In this way, the downloaded IPYNB file contains the outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isUzv1MGmmNq"
      },
      "source": [
        "## 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EilN1rhkmmNq"
      },
      "source": [
        "### 1.1. Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrEC3ztzmmNr",
        "outputId": "fc59472b-d139-444d-9739-b3a2a9137f62"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "import numpy\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('shape of x_train: ' + str(x_train.shape))\n",
        "print('shape of y_train: ' + str(y_train.shape))\n",
        "print('shape of x_test: ' + str(x_test.shape))\n",
        "print('shape of y_test: ' + str(y_test.shape))\n",
        "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of x_train: (50000, 32, 32, 3)\n",
            "shape of y_train: (50000, 1)\n",
            "shape of x_test: (10000, 32, 32, 3)\n",
            "shape of y_test: (10000, 1)\n",
            "number of classes: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNeYJIYKmmNr"
      },
      "source": [
        "### 1.2. One-hot encode the labels\n",
        "\n",
        "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
        "\n",
        "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
        "\n",
        "2. Apply the function to ```y_train``` and ```y_test```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a506N9sYmmNr",
        "outputId": "3cc30c0a-8e11-4b8c-cb2c-19ee9ca40a95"
      },
      "source": [
        "def to_one_hot(y, num_class=10):\n",
        "    # <fill the function>\n",
        "    # ...\n",
        "    results = numpy.zeros((len(y),num_class))\n",
        "    for i, label in enumerate(y):\n",
        "      results[i, label] = 1\n",
        "    return results\n",
        "\n",
        "y_train_vec = to_one_hot(y_train)\n",
        "y_test_vec = to_one_hot(y_test)\n",
        "\n",
        "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
        "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train_vec[0])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of y_train_vec: (50000, 10)\n",
            "Shape of y_test_vec: (10000, 10)\n",
            "[6]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbytBjp_mmNs"
      },
      "source": [
        "#### Remark: the outputs should be\n",
        "* Shape of y_train_vec: (50000, 10)\n",
        "* Shape of y_test_vec: (10000, 10)\n",
        "* [6]\n",
        "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19-2EAvummNs"
      },
      "source": [
        "### 1.3. Randomly partition the training set to training and validation sets\n",
        "\n",
        "Randomly partition the 50K training samples to 2 sets:\n",
        "* a training set containing 40K samples\n",
        "* a validation set containing 10K samples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT3N-GNnmmNs",
        "outputId": "fcae260f-7980-46e8-b64f-4efb9f67fa9f"
      },
      "source": [
        "rand_indices = numpy.random.permutation(50000)\n",
        "train_indices = rand_indices[0:40000]\n",
        "valid_indices = rand_indices[40000:50000]\n",
        "\n",
        "x_val = x_train[valid_indices, :]\n",
        "y_val = y_train_vec[valid_indices, :]\n",
        "\n",
        "x_tr = x_train[train_indices, :]\n",
        "y_tr = y_train_vec[train_indices, :]\n",
        "\n",
        "print('Shape of x_tr: ' + str(x_tr.shape))\n",
        "print('Shape of y_tr: ' + str(y_tr.shape))\n",
        "print('Shape of x_val: ' + str(x_val.shape))\n",
        "print('Shape of y_val: ' + str(y_val.shape))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x_tr: (40000, 32, 32, 3)\n",
            "Shape of y_tr: (40000, 10)\n",
            "Shape of x_val: (10000, 32, 32, 3)\n",
            "Shape of y_val: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihsHFBSCmmNt"
      },
      "source": [
        "## 2. Build a CNN and tune its hyper-parameters\n",
        "\n",
        "1. Build a convolutional neural network model\n",
        "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
        "    * Do NOT use test data for hyper-parameter tuning!!!\n",
        "3. Try to achieve a validation accuracy as high as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er7lSNDjmmNt"
      },
      "source": [
        "### Remark: \n",
        "\n",
        "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
        "* Add more layers.\n",
        "* Use regularizations, e.g., dropout.\n",
        "* Use batch normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZLOGN3_mmNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87caa9bc-a024-4bc1-f9cb-651dd47ae78a"
      },
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(32,(3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(64,(3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(128,(3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 30, 30, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 15, 15, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 13, 13, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 6, 6, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 6, 6, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 4, 4, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 556,586\n",
            "Trainable params: 555,690\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-lW1saAmmNu"
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "learning_rate = 1E-4  \n",
        "# to be tuned!\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4-VrCGHmmNv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5fef42d-a0e1-4417-a063-fb0837238379"
      },
      "source": [
        "history = model.fit(x_tr, y_tr, batch_size=32, epochs=30, validation_data=(x_val, y_val))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1250/1250 [==============================] - 13s 7ms/step - loss: 2.1081 - acc: 0.2771 - val_loss: 1.4540 - val_acc: 0.4768\n",
            "Epoch 2/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 1.4784 - acc: 0.4633 - val_loss: 1.4419 - val_acc: 0.4980\n",
            "Epoch 3/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 1.2800 - acc: 0.5432 - val_loss: 1.2095 - val_acc: 0.5701\n",
            "Epoch 4/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 1.1514 - acc: 0.5895 - val_loss: 1.0719 - val_acc: 0.6243\n",
            "Epoch 5/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 1.0525 - acc: 0.6283 - val_loss: 1.0453 - val_acc: 0.6416\n",
            "Epoch 6/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.9680 - acc: 0.6562 - val_loss: 0.9710 - val_acc: 0.6689\n",
            "Epoch 7/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.9123 - acc: 0.6818 - val_loss: 1.0142 - val_acc: 0.6614\n",
            "Epoch 8/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.8605 - acc: 0.6949 - val_loss: 0.9059 - val_acc: 0.6966\n",
            "Epoch 9/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.8090 - acc: 0.7194 - val_loss: 0.7918 - val_acc: 0.7285\n",
            "Epoch 10/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.7645 - acc: 0.7306 - val_loss: 0.7738 - val_acc: 0.7362\n",
            "Epoch 11/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.7243 - acc: 0.7449 - val_loss: 0.7167 - val_acc: 0.7568\n",
            "Epoch 12/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.7030 - acc: 0.7541 - val_loss: 0.7235 - val_acc: 0.7512\n",
            "Epoch 13/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.6708 - acc: 0.7657 - val_loss: 0.7331 - val_acc: 0.7565\n",
            "Epoch 14/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.6317 - acc: 0.7769 - val_loss: 0.7575 - val_acc: 0.7469\n",
            "Epoch 15/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.6082 - acc: 0.7853 - val_loss: 0.6626 - val_acc: 0.7781\n",
            "Epoch 16/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.5815 - acc: 0.7941 - val_loss: 0.6532 - val_acc: 0.7766\n",
            "Epoch 17/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.5680 - acc: 0.8022 - val_loss: 0.6442 - val_acc: 0.7874\n",
            "Epoch 18/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.5425 - acc: 0.8086 - val_loss: 0.6520 - val_acc: 0.7820\n",
            "Epoch 19/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.5265 - acc: 0.8115 - val_loss: 0.6515 - val_acc: 0.7834\n",
            "Epoch 20/30\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5099 - acc: 0.8189 - val_loss: 0.6748 - val_acc: 0.7800\n",
            "Epoch 21/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.4846 - acc: 0.8270 - val_loss: 0.6325 - val_acc: 0.7964\n",
            "Epoch 22/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.4739 - acc: 0.8331 - val_loss: 0.6452 - val_acc: 0.7885\n",
            "Epoch 23/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.4503 - acc: 0.8415 - val_loss: 0.6462 - val_acc: 0.7912\n",
            "Epoch 24/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.4450 - acc: 0.8405 - val_loss: 0.6273 - val_acc: 0.7954\n",
            "Epoch 25/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.4228 - acc: 0.8509 - val_loss: 0.6374 - val_acc: 0.7931\n",
            "Epoch 26/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.4214 - acc: 0.8506 - val_loss: 0.6015 - val_acc: 0.8030\n",
            "Epoch 27/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.4011 - acc: 0.8619 - val_loss: 0.6990 - val_acc: 0.7778\n",
            "Epoch 28/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.3837 - acc: 0.8657 - val_loss: 0.6276 - val_acc: 0.8016\n",
            "Epoch 29/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.3699 - acc: 0.8680 - val_loss: 0.6586 - val_acc: 0.7935\n",
            "Epoch 30/30\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 0.3651 - acc: 0.8689 - val_loss: 0.6345 - val_acc: 0.8016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfZIa25SmmNv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "0a843647-5d2b-4562-bb3b-623d329011b2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjFyiSiemmNv"
      },
      "source": [
        "## 3. Train (again) and evaluate the model\n",
        "\n",
        "- To this end, you have found the \"best\" hyper-parameters. \n",
        "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
        "- Evaluate your model on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aZfiOSVmmNw"
      },
      "source": [
        "### 3.1. Train the model on the entire training set\n",
        "\n",
        "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBHaZEPammNw"
      },
      "source": [
        "# <Compile your model again (using the same hyper-parameters)>\n",
        "# ...\n",
        "\n",
        "from keras import optimizers\n",
        "\n",
        "learning_rate = 1E-4  \n",
        "# to be tuned!\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7opVD0CmmNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "925e1e5c-329e-4736-fd4d-6e6c8454730a"
      },
      "source": [
        "# <Train your model on the entire training set (50K samples)>\n",
        "# <Use (x_train, y_train_vec) instead of (x_tr, y_tr)>\n",
        "# <Do NOT use the validation_data option (because now you do not have validation data)>\n",
        "# ...\n",
        "history = model.fit(x_train, y_train_vec, batch_size=32, epochs=30)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1563/1563 [==============================] - 11s 6ms/step - loss: 0.4460 - acc: 0.8485\n",
            "Epoch 2/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4125 - acc: 0.8564\n",
            "Epoch 3/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.4016 - acc: 0.8599\n",
            "Epoch 4/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3835 - acc: 0.8638\n",
            "Epoch 5/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3850 - acc: 0.8663\n",
            "Epoch 6/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3674 - acc: 0.8704\n",
            "Epoch 7/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3585 - acc: 0.8746\n",
            "Epoch 8/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3399 - acc: 0.8800\n",
            "Epoch 9/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3282 - acc: 0.8842\n",
            "Epoch 10/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3259 - acc: 0.8847\n",
            "Epoch 11/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3111 - acc: 0.8902\n",
            "Epoch 12/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3047 - acc: 0.8930\n",
            "Epoch 13/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3014 - acc: 0.8933\n",
            "Epoch 14/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2859 - acc: 0.8978\n",
            "Epoch 15/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2796 - acc: 0.9008\n",
            "Epoch 16/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2763 - acc: 0.9001\n",
            "Epoch 17/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2695 - acc: 0.9059\n",
            "Epoch 18/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2590 - acc: 0.9071\n",
            "Epoch 19/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2677 - acc: 0.9073\n",
            "Epoch 20/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2471 - acc: 0.9122\n",
            "Epoch 21/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2468 - acc: 0.9117\n",
            "Epoch 22/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2494 - acc: 0.9116\n",
            "Epoch 23/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2343 - acc: 0.9185\n",
            "Epoch 24/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2392 - acc: 0.9151\n",
            "Epoch 25/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2307 - acc: 0.9197\n",
            "Epoch 26/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2275 - acc: 0.9177\n",
            "Epoch 27/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2159 - acc: 0.9237\n",
            "Epoch 28/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2145 - acc: 0.9250\n",
            "Epoch 29/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2085 - acc: 0.9250\n",
            "Epoch 30/30\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.2070 - acc: 0.9258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tHYfUUEmmNw"
      },
      "source": [
        "### 3.2. Evaluate the model on the test set\n",
        "\n",
        "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf8IUu7lmmNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c649335-0d76-4c3c-d383-6f5d5172a9f2"
      },
      "source": [
        "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(loss_and_acc[0]))\n",
        "print('accuracy = ' + str(loss_and_acc[1]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.6154 - acc: 0.8358\n",
            "loss = 0.6154413819313049\n",
            "accuracy = 0.8357999920845032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cduAlzwDmmNx"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}